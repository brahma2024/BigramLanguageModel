{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsrqpQK3pt00PiTfpe2mq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brahma2024/LLM-study/blob/main/4_Becoming%2BBackprop%2BNinja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4 - Becomming a backprop Ninja**"
      ],
      "metadata": {
        "id": "6YmeANIVttIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "MC8kz7f6YS1k"
      },
      "outputs": [],
      "source": [
        "# No change in code from 3 - Activations&Gradients Lecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "kqSoHR9vlAwe"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/brahma2024/LLM-study/main/names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lznHALpBlQTg",
        "outputId": "46bd1f73-c2d7-4d0c-9abb-c22fa61dea2b"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-19 20:57:08--  https://raw.githubusercontent.com/brahma2024/LLM-study/main/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228146 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.3’\n",
            "\n",
            "names.txt.3         100%[===================>] 222.80K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-06-19 20:57:08 (4.42 MB/s) - ‘names.txt.3’ saved [228146/228146]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extfhiI1lcm3",
        "outputId": "969c92ab-5acc-445a-b30f-503e8507a56f"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of chracters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd8oM0Q5l8FG",
        "outputId": "6473cd56-1369-4365-c508-3dbad397bc7f"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "\n",
        "      # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "# split dataset into train | dev | test\n",
        "\n",
        "import random\n",
        "random.seed(0)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])       # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xtest, Ytest = build_dataset(words[n2:])   # 10%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCU07v1TmX9z",
        "outputId": "e307143a-e5da-4ca0-db2b-be6bad0c0ec5"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182594, 3]) torch.Size([182594])\n",
            "torch.Size([22846, 3]) torch.Size([22846])\n",
            "torch.Size([22706, 3]) torch.Size([22706])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# boilerplate done above, action next"
      ],
      "metadata": {
        "id": "IgdiXkv8sQAO"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()    # comparing all values of dt with t.grad and return a boolean value | if all values are equal returns TRUE\n",
        "  app = torch.allclose(dt, t.grad)       # torch.allclose looks for values which are very very close, say to some nums of decimal, if Yes returns True | allows a little wiggle room\n",
        "  maxdiff = (dt - t.grad).abs().max().item()  # what is the maxim difference if there is any\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "KUIRuDk0u4Zz"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducability\n",
        "C = torch.randn((vocab_size, n_embd), generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # just to check for gradient wrt to b1, otherwise in actual NN training its useless because of batchNorm\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initialiazing many of these parameters in non-standard ways\n",
        "# because sometimes initializing with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0XffmQeUBqf",
        "outputId": "109b2279-dacc-4fd8-9600-7054bc0147d8"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just training one batch\n",
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix =torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) # ix = list of random int between [0, Xtr.shape[0]]\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y"
      ],
      "metadata": {
        "id": "kgD-isz4WEyJ"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embd the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # contcatenate the vectors\n",
        "# Linear Layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer preactivation\n",
        "# BatchNorm Layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear Layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss ( same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values # from the log probs for each character, given a sample/example, fetch the largest log probability + its index\n",
        "norm_logits = logits - logit_maxes # the index with largest prob = 0 rest -ve\n",
        "counts = norm_logits.exp() # for value = 0, count= 1 and for rest 0 < counts < 1\n",
        "counts_sum = counts.sum(1, keepdim=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0/count_sum) instead then I can't backprop to be exact ..\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean() # reduce the loglikelihood to as small as zero\n",
        "\n",
        "# pytorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts_sum_inv, counts_sum, counts,\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "          bnvar_inv, bnvar, bndiff2, bndiff, bnmeani, hprebn,\n",
        "          embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S3YFx3bvlQx",
        "outputId": "e40667c7-b313-4759-9279-c241c241b8a1"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.2823, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ixl = logits.max(1, keepdim=False).indices\n",
        "dlogits1 = torch.zeros_like(logits)\n",
        "dlogits1[torch.arange(n),ixl] = 1.0\n",
        "\n",
        "# all of the options below graph the same output\n",
        "# plt.imshow(F.one_hot(ixl, num_classes=logits.shape[1]))\n",
        "plt.imshow(dlogits1) # shows which indexes have max\n",
        "# plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1])) # where all values are 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "NXZa4iDGbRkr",
        "outputId": "e0e07b34-a0f6-4c38-cf92-e7a7640dc7a0"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7d2d30a64f70>"
            ]
          },
          "metadata": {},
          "execution_count": 165
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Code: To verify the mathematical steps behind calculating the cross-entropy loss\n",
        "# !pip install torchviz\n",
        "# from torchviz import make_dot\n",
        "\n",
        "# # Seed for reproducibility\n",
        "# torch.manual_seed(0)\n",
        "\n",
        "# # Example tensor calculations\n",
        "# a = torch.randint(-100, 100, (10,5), dtype=torch.float, requires_grad=True)  # logits\n",
        "# out = torch.randint(0,5,(5,))  # Yb\n",
        "# a_max = a.max(1, keepdim=True).values  # logits_max\n",
        "# b = a - a_max  # norm_logits\n",
        "# b_count = b.exp()  # exponential of normalized logits\n",
        "# b_count_sum = b_count.sum(1, keepdim=True)  # sum of exponentials\n",
        "# b_count_sum_inv = b_count_sum.pow(-1)  # inverse of sum\n",
        "# b_probs = b_count * b_count_sum_inv  # probabilities\n",
        "# log_probs = b_probs.log()  # log probabilities\n",
        "# loss = -(log_probs[torch.arange(5), out]).mean()  # cross-entropy loss\n",
        "\n",
        "# # Generate graph\n",
        "# # graph = make_dot(loss, params={'a': a})\n",
        "# # graph.attr(rankdir='LR', size='24, 24')  # Set the graph to be left-to-right\n",
        "# # graph.render(\"output_horizontal\", format=\"png\", cleanup=True)\n",
        "# # graph\n",
        "\n",
        "# from graphviz import Digraph\n",
        "\n",
        "# # Create a new directed graph\n",
        "# dot = Digraph(format='png')\n",
        "# dot.attr(rankdir='LR')  # Left to Right graph\n",
        "\n",
        "# # Adding nodes representing each step in your tensor operations\n",
        "# dot.node('A', 'a = rand([-100, 100], [10,5])')\n",
        "# dot.node('B', 'out = rand([0,5], [5,])')\n",
        "# dot.node('C', 'a_max = max(a)')\n",
        "# dot.node('D', 'b = a - a_max')\n",
        "# dot.node('E', 'b_count = exp(b)')\n",
        "# dot.node('F', 'b_count_sum = sum(b_count)')\n",
        "# dot.node('G', 'b_count_sum_inv = 1 / b_count_sum')\n",
        "# dot.node('H', 'b_probs = b_count * b_count_sum_inv')\n",
        "# dot.node('I', 'log_probs = log(b_probs)')\n",
        "# dot.node('J', 'loss = -mean(log_probs)')\n",
        "\n",
        "# # Adding edges\n",
        "# dot.edge('A', 'C', 'Calculate max per row')\n",
        "# dot.edge('A', 'D', 'Subtract max')\n",
        "# dot.edge('D', 'E', 'Exponential')\n",
        "# dot.edge('E', 'F', 'Sum of exponentials')\n",
        "# dot.edge('F', 'G', 'Inverse')\n",
        "# dot.edge('E', 'H', 'Element-wise multiplication')\n",
        "# dot.edge('G', 'H')\n",
        "# dot.edge('H', 'I', 'Logarithm')\n",
        "# dot.edge('I', 'J', 'Negative mean')\n",
        "\n",
        "# # Render the graph to a file\n",
        "# dot.render('tensor_operations', view=True)\n",
        "\n",
        "# # Display the graph (if using Jupyter, for instance)\n",
        "# dot\n"
      ],
      "metadata": {
        "id": "Racz_KbZvZlq",
        "cellView": "form"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "# --------------\n",
        "# Calculate the Jacobian Matrix at each step\n",
        "# --------------\n",
        "# derivative of loss w.r.t. all elements of logprobs\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0/n      # for all indexes = Yb will be - 1/n derivative of mean\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "\n",
        "dprobs = (1.0 / probs) * dlogprobs # derivative of loss w.r.t. all elements of probs\n",
        "cmp('probs', dprobs, probs)\n",
        "# how does this works:\n",
        "# if probs = 1.0 that is most characters are getting correctly generated in output so 1/probs = 1\n",
        "# that means dlogprobs i.e. the gradients of loss wrt to logprobs gets a passthrough\n",
        "# but if probs are incorrectly assigned, i.e. probs is very low,\n",
        "# then 1/probs will be very large and will boost the dlogprobs to make the model learn fast\n",
        "\n",
        "# probs = counts * counts_sum_inv  | i.e. we need to find the derivative both with respect to counts & counts_sum_inv\n",
        "# 1: dcounts_sum_inv i.e. change in loss with respect to count_sum_inv\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "# here the node counts_sum_inv is used multiple times, and in the backward pass all the gradients from 27 (vocab_size) characters arrive at\n",
        "# one node for each 32 example, so correct thing to do is to sum all the gradients at that node and maintain the shape\n",
        "# counts_sum_inv is of shape [n, 1] so its gradient should be the same shape hence post summing, keepdim = True\n",
        "\n",
        "# 2: dcounts i.e. dloss/dcounts | this is part 1 of dloss/dcount\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "\n",
        "# now calculating dloss/dcounts_sum\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "\n",
        "# dloss/ dcounts | this is total dloss/dcount\n",
        "# count node: impacts changes in probs and in counts_sum | so both these changes need to be added as part of the back propagation\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum # change in loss w.r.t. counts [chain rule applied]\n",
        "cmp('counts', dcounts, counts)\n",
        "\n",
        "# dnorm_logits = dcounts * norm_logits.exp() # dloss / dnorm_logits | counts = dnorm_logits.exp()\n",
        "dnorm_logits = counts * dcounts\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "\n",
        "# to allow for gradient pass through norm_logits = logits - logit_maxes | its in 2 parts\n",
        "# 1: logit_maxes\n",
        "# dlogit_maxes = (dnorm_logits * -torch.ones_like(logit_maxes)).sum(1, keepdim=True) # dloss/ dlogit_maxes | this is expensive since we are creating a new tensor with all ones\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True) # same as above, just a more neater implementation\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "# Just a side note: gradients of logits_max does not matter much when computing final loss\n",
        "# reason: purpose of logit_max is only from the purpose of numerical stability and hence does not contribute much to the final loss\n",
        "# print(f'{dlogit_maxes=}') # printing to see values of dlogits_maxes ~ 0.0\n",
        "\n",
        "# 2: dlogits | this is the 1 part of the dlogits\n",
        "# dlogits = dnorm_logits * torch.ones_like(logits) # expensive option since we are creating a new tensor with all ones\n",
        "dlogits = dnorm_logits.clone()\n",
        "\n",
        "# 2nd part of dlogits\n",
        "# for logits.max(1, keepdim=True).values | what its doing is picking up the values with maximum value for each of n example sets\n",
        "# method 1: the usual approach as used above\n",
        "# dlogits1 = torch.zeros_like(logits) # all values initialized to zero\n",
        "# dlogits1[range(n),logits.max(1, keepdim=False).indices] = 1.0 # only values at indices which had maximum value in fwd pass are assigned 1.0\n",
        "# dlogits += dlogit_maxes * dlogits1 # gradient flowing from logit_maxes to logits is added to gradient flowing from node norm_logits\n",
        "# method 2: using F.one_hot\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes # at indices selected one_hot will make value = 1, rest = 0\n",
        "cmp('logits', dlogits, logits)\n",
        "\n",
        "# gardient flow through activation layer logits = h @ W1 + b1\n",
        "# dloss/dh\n",
        "dh = dlogits @ W2.T # W2.t() = W2.T\n",
        "cmp('h', dh, h)\n",
        "\n",
        "# dloss/dW2\n",
        "dW2 = h.T @ dlogits # h.t() = h.T\n",
        "# print(dW2, dW2.t().shape, W2.shape)\n",
        "cmp('W2', dW2, W2)\n",
        "\n",
        "# dloss/db2\n",
        "# db2 = (dlogits * torch.ones_like(b2)).sum(0, keepdim=True)\n",
        "# db2 = (dlogits).sum(0, keepdim=True) # same but simpler implementation\n",
        "db2 = (dlogits).sum(0, keepdim=False) # b2 is a 1D vector, hence heepdim=False\n",
        "cmp('b2', db2, b2)\n",
        "\n",
        "\n",
        "# dloss/dhpreact = 4/(e^(-x) + e^x)^2 | dtanh(x)^2 = sech(x)^ = 1 - tanh(x)^2\n",
        "# dhpreact = dh * torch.sub(torch.ones_like(hpreact), (torch.tanh(hpreact)**2))\n",
        "dhpreact = (1.0 - h**2) * dh # same as above, but simpler/easier to read\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "\n",
        "# dloss/dbngain\n",
        "dbngain = (dhpreact * bnraw).sum(0, keepdim=True) # bngain was broadcasted to all 32 rows(examples), so the gradient is summed up in that direction\n",
        "cmp('bngain', dbngain, bngain)\n",
        "\n",
        "# dloss/dbnraw\n",
        "dbnraw = bngain * dhpreact\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "\n",
        "# dloss/dbnbias\n",
        "# dbnbias = (dhpreact * torch.ones_like(bnbias)).sum(0, keepdim=True)\n",
        "dbnbias = (dhpreact).sum(0, keepdim=True) # bnbias was broadcasted to all 32 rows(examples), so the gradient is summed up in that direction\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "\n",
        "# dloss/dbnvar_inv\n",
        "# Ref.: bnraw = bndiff * bnvar_inv\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "\n",
        "# dloss/dbndiff: Part 1\n",
        "dbndiff = bnvar_inv * dbnraw # bndiff part 1: this is 1st part of the gradient flowing through bndiff, part 2 when calc gradient flowing through bndiff2\n",
        "# cmp('bndiff: Part 1', dbndiff, bndiff)\n",
        "\n",
        "# dloss/dbnvar\n",
        "# Ref.:bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "dbnvar = dbnvar_inv * -(0.5 * (bnvar + 1e-5)**-1.5)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "\n",
        "# dloss/dbndiff2\n",
        "# Ref.:bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
        "# dbndiff2 = 1.0/(n-1) * dbnvar\n",
        "dbndiff2 = ((1.0/(n-1)) * torch.ones_like(bndiff2)) * dbnvar\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "\n",
        "# dloss/dbndiff: Part 2\n",
        "# Ref.:bndiff2 = bndiff**2\n",
        "dbndiff += 2 * bndiff * dbndiff2\n",
        "cmp('bndiff: Full', dbndiff, bndiff)\n",
        "\n",
        "# dloss/dhprebn: Part 1\n",
        "# Ref.: bndiff = hprebn - bnmeani\n",
        "dhprebn = dbndiff.clone()\n",
        "# cmp('hprebn: Part 1', dhprebn, hprebn)\n",
        "\n",
        "# dloss /dbnmeani\n",
        "# Ref: bndiff = hprebn - bnmeani\n",
        "# dbnmeani = -(torch.ones_like(bndiff) * dbndiff).sum(0, keepdim=True)\n",
        "dbnmeani = (-dbndiff).sum(0, keepdim=True)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "\n",
        "# dloss/dhprebn: Part 2\n",
        "# Ref.:bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "dhprebn += 1/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "# dhprebn += 1/n * (dbnmeani).sum(0, keepdim=True)  # this is another approach\n",
        "cmp('hprebn: Full', dhprebn, hprebn)\n",
        "\n",
        "# dloss/dembcat\n",
        "# Ref: hprebn = embcat @ W1 + b1\n",
        "dembcat = dhprebn @ W1.T\n",
        "cmp('embcat', dembcat, embcat)\n",
        "\n",
        "# dloss/dW1\n",
        "dW1 = embcat.T @ dhprebn\n",
        "cmp('W1', dW1, W1)\n",
        "\n",
        "# dloss/db1\n",
        "db1 = dhprebn.sum(0, keepdim=False) # keepdim=False, since b1 is a 1D tensor\n",
        "cmp('b1', db1, b1)\n",
        "\n",
        "# dloss/demb\n",
        "# Ref:embcat = emb.view(emb.shape[0], -1)\n",
        "# demb = dembcat.view(emb.shape[0], emb.shape[1], emb.shape[2]) # more not-so-sensible version\n",
        "demb = dembcat.view(emb.shape)\n",
        "cmp('emb', demb, emb)\n",
        "\n",
        "# dloss/demb\n",
        "# Ref: emb = C[Xb]\n",
        "# Explanation: Here what C[Xb] is actually doing is, for all examples in input batch Xb\n",
        "# it looks up into the embedding table C, and pulls out the embedding vector corresponding to that itos map (a = 1, .=0, and so on)\n",
        "# Now when passing the gradients through C[Xb], it might be the possibility that one index position in embedding table C is called multiple time\n",
        "# e.g. print(Xb[:5])\n",
        "# returns tensor([[ 0,  0,  0],\n",
        "#        [ 1, 14,  7],\n",
        "        # [ 0,  0, 11],\n",
        "        # [18,  9,  5],\n",
        "        # [18,  9,  1]]) ---> like 0, 1, 9, 18 are referenced multiple times\n",
        "# so the gradient for these needs to added together\n",
        "# this can be done using for loop\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    xkj = Xb[k,j]\n",
        "    dC[xkj] += demb[k,j]\n",
        "\n",
        "cmp('C', dC, C)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uWRdZPHHxNY",
        "outputId": "f8165b4e-5ceb-49a0-d86c-57bbc09f828f"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.546585164964199e-11\n",
            "bndiff: Full    | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "hprebn: Full    | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(emb.shape[0]):\n",
        "  for j in range(emb.shape[1]):\n",
        "    # print(i,j, '---', C[Xb][i,j] == emb[i, j])\n",
        "    ixt = Xb[i,j]\n",
        "    print(ixt, '---> ', C[ixt], '--->', C[Xb][i,j])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx5HXYU3x8bB",
        "outputId": "7c12f153-5ad8-43db-a3e7-3a4f1b3a5a67"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(14) --->  tensor([ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
            "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
            "       grad_fn=<SelectBackward0>) ---> tensor([ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
            "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor(7) --->  tensor([ 0.6169,  1.5160, -1.0447, -0.6641, -0.7239,  1.7507,  0.1753,  0.9928,\n",
            "        -0.6279,  0.0770], grad_fn=<SelectBackward0>) ---> tensor([ 0.6169,  1.5160, -1.0447, -0.6641, -0.7239,  1.7507,  0.1753,  0.9928,\n",
            "        -0.6279,  0.0770], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(11) --->  tensor([-0.5653,  0.5428,  0.1755, -2.2901, -0.7093, -0.2928, -2.1803,  0.0793,\n",
            "         0.9019,  1.2028], grad_fn=<SelectBackward0>) ---> tensor([-0.5653,  0.5428,  0.1755, -2.2901, -0.7093, -0.2928, -2.1803,  0.0793,\n",
            "         0.9019,  1.2028], grad_fn=<SelectBackward0>)\n",
            "tensor(18) --->  tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>) ---> tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>)\n",
            "tensor(9) --->  tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>) ---> tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>)\n",
            "tensor(5) --->  tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>) ---> tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>)\n",
            "tensor(18) --->  tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>) ---> tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>)\n",
            "tensor(9) --->  tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>) ---> tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(15) --->  tensor([ 0.5557,  0.4746, -1.3867,  1.6229,  0.1720,  0.9885,  0.5066,  1.0198,\n",
            "        -1.9062, -0.4275], grad_fn=<SelectBackward0>) ---> tensor([ 0.5557,  0.4746, -1.3867,  1.6229,  0.1720,  0.9885,  0.5066,  1.0198,\n",
            "        -1.9062, -0.4275], grad_fn=<SelectBackward0>)\n",
            "tensor(12) --->  tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>) ---> tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(14) --->  tensor([ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
            "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
            "       grad_fn=<SelectBackward0>) ---> tensor([ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
            "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(18) --->  tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>) ---> tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>)\n",
            "tensor(9) --->  tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>) ---> tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(8) --->  tensor([-1.1641,  1.2473, -0.2706, -1.3635,  1.3066,  0.3231,  1.0358, -0.8625,\n",
            "        -1.2575,  0.9418], grad_fn=<SelectBackward0>) ---> tensor([-1.1641,  1.2473, -0.2706, -1.3635,  1.3066,  0.3231,  1.0358, -0.8625,\n",
            "        -1.2575,  0.9418], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(12) --->  tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>) ---> tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>)\n",
            "tensor(9) --->  tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>) ---> tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>)\n",
            "tensor(19) --->  tensor([-0.2129,  0.5095,  0.3271,  1.9661, -0.2409, -0.7952,  0.2720, -1.1100,\n",
            "        -0.4528, -0.4958], grad_fn=<SelectBackward0>) ---> tensor([-0.2129,  0.5095,  0.3271,  1.9661, -0.2409, -0.7952,  0.2720, -1.1100,\n",
            "        -0.4528, -0.4958], grad_fn=<SelectBackward0>)\n",
            "tensor(12) --->  tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>) ---> tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>)\n",
            "tensor(23) --->  tensor([-1.0527, -0.1437, -0.2774,  1.1634, -0.6691,  0.6492,  0.5824,  1.9264,\n",
            "        -0.3785,  0.0080], grad_fn=<SelectBackward0>) ---> tensor([-1.0527, -0.1437, -0.2774,  1.1634, -0.6691,  0.6492,  0.5824,  1.9264,\n",
            "        -0.3785,  0.0080], grad_fn=<SelectBackward0>)\n",
            "tensor(5) --->  tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>) ---> tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>)\n",
            "tensor(14) --->  tensor([ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
            "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
            "       grad_fn=<SelectBackward0>) ---> tensor([ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
            "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(13) --->  tensor([-0.3111, -0.3060, -1.7495, -1.6335,  0.3876,  0.4724,  1.4830,  0.3175,\n",
            "         1.0588,  2.3982], grad_fn=<SelectBackward0>) ---> tensor([-0.3111, -0.3060, -1.7495, -1.6335,  0.3876,  0.4724,  1.4830,  0.3175,\n",
            "         1.0588,  2.3982], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(11) --->  tensor([-0.5653,  0.5428,  0.1755, -2.2901, -0.7093, -0.2928, -2.1803,  0.0793,\n",
            "         0.9019,  1.2028], grad_fn=<SelectBackward0>) ---> tensor([-0.5653,  0.5428,  0.1755, -2.2901, -0.7093, -0.2928, -2.1803,  0.0793,\n",
            "         0.9019,  1.2028], grad_fn=<SelectBackward0>)\n",
            "tensor(13) --->  tensor([-0.3111, -0.3060, -1.7495, -1.6335,  0.3876,  0.4724,  1.4830,  0.3175,\n",
            "         1.0588,  2.3982], grad_fn=<SelectBackward0>) ---> tensor([-0.3111, -0.3060, -1.7495, -1.6335,  0.3876,  0.4724,  1.4830,  0.3175,\n",
            "         1.0588,  2.3982], grad_fn=<SelectBackward0>)\n",
            "tensor(9) --->  tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>) ---> tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>)\n",
            "tensor(12) --->  tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>) ---> tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>)\n",
            "tensor(23) --->  tensor([-1.0527, -0.1437, -0.2774,  1.1634, -0.6691,  0.6492,  0.5824,  1.9264,\n",
            "        -0.3785,  0.0080], grad_fn=<SelectBackward0>) ---> tensor([-1.0527, -0.1437, -0.2774,  1.1634, -0.6691,  0.6492,  0.5824,  1.9264,\n",
            "        -0.3785,  0.0080], grad_fn=<SelectBackward0>)\n",
            "tensor(25) --->  tensor([-1.2801,  0.0924,  0.1053, -0.3907,  0.0317, -0.5475,  0.8183, -0.8163,\n",
            "        -0.3924, -0.7452], grad_fn=<SelectBackward0>) ---> tensor([-1.2801,  0.0924,  0.1053, -0.3907,  0.0317, -0.5475,  0.8183, -0.8163,\n",
            "        -0.3924, -0.7452], grad_fn=<SelectBackward0>)\n",
            "tensor(14) --->  tensor([ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
            "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
            "       grad_fn=<SelectBackward0>) ---> tensor([ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
            "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(20) --->  tensor([ 1.2648,  1.4625,  1.1199,  0.9954, -1.2353,  0.7382,  0.8142, -0.7381,\n",
            "         0.5671, -1.4601], grad_fn=<SelectBackward0>) ---> tensor([ 1.2648,  1.4625,  1.1199,  0.9954, -1.2353,  0.7382,  0.8142, -0.7381,\n",
            "         0.5671, -1.4601], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(12) --->  tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>) ---> tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>)\n",
            "tensor(5) --->  tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>) ---> tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(12) --->  tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>) ---> tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
            "         0.5642,  0.9684], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(26) --->  tensor([-0.9465, -0.1594, -0.1934, -0.3766, -0.0492,  0.0939, -0.6453,  1.2108,\n",
            "        -0.7820,  0.3845], grad_fn=<SelectBackward0>) ---> tensor([-0.9465, -0.1594, -0.1934, -0.3766, -0.0492,  0.0939, -0.6453,  1.2108,\n",
            "        -0.7820,  0.3845], grad_fn=<SelectBackward0>)\n",
            "tensor(3) --->  tensor([-1.0725,  0.7276,  0.0511,  1.3095, -0.8022, -0.8504, -1.8068,  1.2523,\n",
            "        -1.2256,  1.2165], grad_fn=<SelectBackward0>) ---> tensor([-1.0725,  0.7276,  0.0511,  1.3095, -0.8022, -0.8504, -1.8068,  1.2523,\n",
            "        -1.2256,  1.2165], grad_fn=<SelectBackward0>)\n",
            "tensor(9) --->  tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>) ---> tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(18) --->  tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>) ---> tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(8) --->  tensor([-1.1641,  1.2473, -0.2706, -1.3635,  1.3066,  0.3231,  1.0358, -0.8625,\n",
            "        -1.2575,  0.9418], grad_fn=<SelectBackward0>) ---> tensor([-1.1641,  1.2473, -0.2706, -1.3635,  1.3066,  0.3231,  1.0358, -0.8625,\n",
            "        -1.2575,  0.9418], grad_fn=<SelectBackward0>)\n",
            "tensor(26) --->  tensor([-0.9465, -0.1594, -0.1934, -0.3766, -0.0492,  0.0939, -0.6453,  1.2108,\n",
            "        -0.7820,  0.3845], grad_fn=<SelectBackward0>) ---> tensor([-0.9465, -0.1594, -0.1934, -0.3766, -0.0492,  0.0939, -0.6453,  1.2108,\n",
            "        -0.7820,  0.3845], grad_fn=<SelectBackward0>)\n",
            "tensor(9) --->  tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>) ---> tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>)\n",
            "tensor(18) --->  tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>) ---> tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>)\n",
            "tensor(18) --->  tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>) ---> tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>)\n",
            "tensor(5) --->  tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>) ---> tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>)\n",
            "tensor(9) --->  tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>) ---> tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>)\n",
            "tensor(15) --->  tensor([ 0.5557,  0.4746, -1.3867,  1.6229,  0.1720,  0.9885,  0.5066,  1.0198,\n",
            "        -1.9062, -0.4275], grad_fn=<SelectBackward0>) ---> tensor([ 0.5557,  0.4746, -1.3867,  1.6229,  0.1720,  0.9885,  0.5066,  1.0198,\n",
            "        -1.9062, -0.4275], grad_fn=<SelectBackward0>)\n",
            "tensor(13) --->  tensor([-0.3111, -0.3060, -1.7495, -1.6335,  0.3876,  0.4724,  1.4830,  0.3175,\n",
            "         1.0588,  2.3982], grad_fn=<SelectBackward0>) ---> tensor([-0.3111, -0.3060, -1.7495, -1.6335,  0.3876,  0.4724,  1.4830,  0.3175,\n",
            "         1.0588,  2.3982], grad_fn=<SelectBackward0>)\n",
            "tensor(9) --->  tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>) ---> tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(5) --->  tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>) ---> tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>)\n",
            "tensor(9) --->  tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>) ---> tensor([-1.3257,  0.1467,  0.1691, -1.5397, -0.7276,  1.1491, -0.8746, -0.2977,\n",
            "        -1.3707,  0.1150], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(1) --->  tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>) ---> tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
            "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
            "tensor(4) --->  tensor([-0.9648, -0.2321, -0.3476,  0.3324, -1.3263,  1.1224,  0.5964,  0.4585,\n",
            "         0.0540, -1.7400], grad_fn=<SelectBackward0>) ---> tensor([-0.9648, -0.2321, -0.3476,  0.3324, -1.3263,  1.1224,  0.5964,  0.4585,\n",
            "         0.0540, -1.7400], grad_fn=<SelectBackward0>)\n",
            "tensor(18) --->  tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>) ---> tensor([ 1.2815, -0.6318, -1.2464,  0.6830, -0.3946,  0.0144,  0.5722,  0.8673,\n",
            "         0.6315, -1.2230], grad_fn=<SelectBackward0>)\n",
            "tensor(5) --->  tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>) ---> tensor([ 0.1156,  0.8032,  0.5411, -1.1646,  0.1476, -1.0006,  0.3801,  0.4733,\n",
            "        -0.9103, -0.7830], grad_fn=<SelectBackward0>)\n",
            "tensor(14) --->  tensor([ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
            "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
            "       grad_fn=<SelectBackward0>) ---> tensor([ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
            "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n",
            "tensor(0) --->  tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>) ---> tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
            "         0.0791,  0.9046], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dC = torch.zeros_like(C)\n",
        "dC[Xb].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3K_92GX4snc",
        "outputId": "3381d226-bc71-49d6-fa79-ce856555897d"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb.shape, C.shape, C[Xb][1,0], C.grad.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd977YlizSYg",
        "outputId": "bae46d26-ec48-4555-bdff-b8f23c3039bc"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 3, 10]),\n",
              " torch.Size([27, 10]),\n",
              " tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
              "          0.6772, -0.8404], grad_fn=<SelectBackward0>),\n",
              " torch.Size([27, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Multiply | Element-wise multiplication between matrices\n",
        "**Matrix Multiply**\n",
        "between matrices A & B = dot product between A-rows and B-columns\n",
        "\n",
        "**Element-wise mulitplication**\n",
        "Each element\n",
        "𝑎\n",
        "𝑖\n",
        "𝑗\n",
        "a\n",
        "ij\n",
        "​\n",
        "  from the broadcasted tensor a is multiplied with the corresponding element\n",
        "𝑏\n",
        "𝑖\n",
        "𝑗\n",
        "b\n",
        "ij\n",
        "​\n",
        "  from tensor b.\n",
        "Result is a new tensor c of shape (32, 64) where each element\n",
        "𝑐\n",
        "𝑖\n",
        "𝑗\n",
        "=\n",
        "𝑎\n",
        "𝑖\n",
        "𝑗\n",
        "×\n",
        "𝑏\n",
        "𝑖\n",
        "𝑗\n",
        "c\n",
        "ij\n",
        "​\n",
        " =a\n",
        "ij\n",
        "​\n",
        " ×b\n",
        "ij\n",
        "​\n",
        ""
      ],
      "metadata": {
        "id": "sT0-cehzCpB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 2: backprop thorugh cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "# norm_logits = logits - logit_maxes\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdim=True)\n",
        "# counts_sum_inv = counts_sum**-1\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii1FDPj4Z44P",
        "outputId": "407b7b64-9cee-4f84-f705-59b141859608"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.282268524169922 diff: 2.384185791015625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward_pass\n",
        "\n",
        "# Here we reduce all the work we did in the steps below, as part of Exercise 1, into a single line\n",
        "# dlogprobs = torch.zeros_like(logprobs)\n",
        "# dlogprobs[range(n), Yb] = -1.0/n\n",
        "# dprobs = (1.0 / probs) * dlogprobs\n",
        "# dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "# dcounts = counts_sum_inv * dprobs\n",
        "# dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "# dnorm_logits = counts * dcounts\n",
        "# dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
        "# dlogits = dnorm_logits.clone()\n",
        "# dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "\n",
        "# First: Reduce this loss function into a simple single line loss function\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdim=True)\n",
        "# counts_sum_inv = counts_sum**-1\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "loss_softmax = - (torch.exp(logits)/(torch.exp(logits).sum(1, keepdim=True)))[range(n), Yb].log().mean()\n",
        "\n",
        "print(loss_softmax.item(), 'diff:', (loss_softmax - loss).item())\n",
        "# Next: Differentiate through it\n",
        "dlogits = (torch.exp(logits)/(torch.exp(logits).sum(1, keepdim=True)))\n",
        "dlogits[range(n), Yb] -= 1.0\n",
        "dlogits /= n\n",
        "\n",
        "# Understanding the gradient calculation for loss  wrt to logits i.e. dloss/dlogits\n",
        "# Seperating out the 2 conditions: if the index inside logits = Yb (output label) | index inside logits != Yb (output label)\n",
        "# in Condition 1: gradient of the loss wrt to the index inside logits = Yb, is = (softmax - 1.0)/n\n",
        "# in Condition 2: gradient of the loss wrt to the index inside logits != Yb (output label), is = softmax/n\n",
        "# where division by n counters the sum across n batches in the forward pass\n",
        "\n",
        "# Another implementation\n",
        "# dlogits = F.softmax(logits, 1)\n",
        "# dlogits[range(n), Yb] -= 1.0\n",
        "# dlogits /= n\n",
        "cmp('logits', dlogits, logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8-4Jlm3dmsB",
        "outputId": "c739684e-2c43-49cf-9164-3dedfc07603b"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.2822682857513428 diff: 0.0\n",
            "logits          | exact: False | approximate: True  | maxdiff: 7.916241884231567e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the probabilities of the logits\n",
        "# and just taking the 1st row\n",
        "F.softmax(logits, 1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o_ao06edzf1",
        "outputId": "0a6f7020-4eb3-4ac7-b0e4-7c388f64753b"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0178, 0.0145, 0.0508, 0.0161, 0.0339, 0.0174, 0.0816, 0.0371, 0.0508,\n",
              "        0.0499, 0.0216, 0.0309, 0.0339, 0.0335, 0.0205, 0.0525, 0.0615, 0.0487,\n",
              "        0.0381, 0.0218, 0.0399, 0.0339, 0.0954, 0.0104, 0.0229, 0.0347, 0.0299],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if we take the gradient of the 1st row * multiple by n to undo the scaling\n",
        "# we get the actual probabilities\n",
        "dlogits[0] * n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5WBt75RZUxn",
        "outputId": "eb9a9afe-90e2-4c86-be9a-b6a8499b76ce"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0178,  0.0145,  0.0508,  0.0161,  0.0339, -0.9826,  0.0816,  0.0371,\n",
              "         0.0508,  0.0499,  0.0216,  0.0309,  0.0339,  0.0335,  0.0205,  0.0525,\n",
              "         0.0615,  0.0487,  0.0381,  0.0218,  0.0399,  0.0339,  0.0954,  0.0104,\n",
              "         0.0229,  0.0347,  0.0299], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and it can be noticed that the only for the index = Yb has a ~ -1.0\n",
        "# what does that mean that the gradient is putting a collective effort to reduce the\n",
        "# and sum of dlogits should =~ 1.0\n",
        "dlogits.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQKA_Hj8iYvB",
        "outputId": "898dbd03-30c5-449a-f9b7-6884cbfd6920"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-3.7253e-09, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this can be thought of as the gradient at each cell as a force\n",
        "# since during training gradient is applied as - leraning_rate * gradient\n",
        "# its trying to make the logit of the correct index/cell as close to 0 as possible so that e^0 = 1 i.e. gets a higher probability\n",
        "# and is trying to pull the values of the other cells/indexes down, for a given example\n",
        "# with the collective force summing up to 0.0"
      ],
      "metadata": {
        "id": "VQrvwbMJijSD"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')\n",
        "\n",
        "# black squares are the indices where we performed -= 1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "4eUXObBgdVkW",
        "outputId": "b3c769c9-acda-40da-e9ef-72767d0252b8"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7d2d307f5570>"
            ]
          },
          "metadata": {},
          "execution_count": 177
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take a derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# Before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# Now:\n",
        "# # Mthod 1:\n",
        "# bndiff_fast = (hprebn - hprebn.mean(0, keepdim=True))\n",
        "# print('bndiff_fast', 'diff:', torch.all(bndiff_fast == bndiff))\n",
        "\n",
        "# bnvar_fast = ((1/(n-1))*(((hprebn - hprebn.mean(0, keepdim=True))**2).sum(0, keepdim=True)))\n",
        "# print('bnvar_fast', 'diff:', torch.all(bnvar_fast == bnvar))\n",
        "\n",
        "# bnraw_fast_1 = bndiff_fast * (bnvar_fast + 1e-5)**-0.5\n",
        "# print('bnraw_fast_1', 'diff:', torch.all(bnraw_fast_1 == bnraw))\n",
        "\n",
        "# bnraw_fast = (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(((1/(n-1))*(((hprebn - hprebn.mean(0, keepdim=True))**2).sum(0, keepdim=True))) + 1e-5)\n",
        "\n",
        "# Method 2:\n",
        "bnraw_fast = (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, correction=1))\n",
        "print('bnraw_fast', 'diff:', torch.all(bnraw_fast == bnraw))\n",
        "\n",
        "hpreact_fast = bngain * bnraw_fast + bnbias\n",
        "print('hpreact_fast', 'diff:', torch.all(hpreact_fast == hpreact))\n",
        "\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTZLypkvJU7x",
        "outputId": "e4d230e9-5383-401b-c7d9-280df3169082"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bnraw_fast diff: tensor(False)\n",
            "hpreact_fast diff: tensor(False)\n",
            "max diff: tensor(2.5749e-05, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hprebn.var(0, keepdim=True), bnvar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4Y785p7RUsC",
        "outputId": "607010e5-d65e-4bfe-9496-ea7d7bc3bf4a"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[3.1335, 2.0495, 2.3350, 1.2706, 5.2063, 1.9729, 2.5762, 2.9609, 0.8553,\n",
              "          0.9468, 1.2785, 1.8114, 3.3689, 1.6224, 0.7143, 2.3032, 3.3543, 6.5057,\n",
              "          1.3057, 4.4658, 3.5887, 1.7295, 3.0513, 2.4652, 1.2883, 2.9347, 2.0102,\n",
              "          1.7779, 3.4728, 2.0461, 1.9141, 2.4270, 2.6702, 2.4235, 3.0248, 2.4040,\n",
              "          2.9726, 3.6039, 0.3378, 1.2289, 0.8259, 3.4072, 2.2100, 1.4483, 2.1420,\n",
              "          1.8644, 2.2749, 1.6203, 1.0987, 2.6631, 2.1233, 0.9670, 2.5863, 4.7339,\n",
              "          1.7726, 3.4225, 1.3726, 1.6840, 2.6751, 0.8119, 2.7571, 1.9284, 2.6743,\n",
              "          2.2856]], grad_fn=<VarBackward0>),\n",
              " tensor([[3.1335, 2.0495, 2.3350, 1.2706, 5.2063, 1.9729, 2.5762, 2.9609, 0.8553,\n",
              "          0.9468, 1.2785, 1.8114, 3.3689, 1.6224, 0.7143, 2.3032, 3.3543, 6.5057,\n",
              "          1.3057, 4.4658, 3.5887, 1.7295, 3.0513, 2.4652, 1.2883, 2.9347, 2.0102,\n",
              "          1.7779, 3.4728, 2.0461, 1.9141, 2.4270, 2.6702, 2.4235, 3.0248, 2.4040,\n",
              "          2.9726, 3.6039, 0.3378, 1.2289, 0.8259, 3.4072, 2.2100, 1.4483, 2.1420,\n",
              "          1.8644, 2.2749, 1.6203, 1.0987, 2.6631, 2.1233, 0.9670, 2.5863, 4.7339,\n",
              "          1.7726, 3.4225, 1.3726, 1.6840, 2.6751, 0.8119, 2.7571, 1.9284, 2.6743,\n",
              "          2.2856]], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass over batchnorm\n",
        "\n",
        "# Before:\n",
        "# dhpreact = (1.0 - h**2) * dh\n",
        "# dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbnbias = (dhpreact).sum(0, keepdim=True) # bnbias was broadcasted to all 32 rows(examples), so the gradient is summed up in that direction\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar = dbnvar_inv * -(0.5 * (bnvar + 1e-5)**-1.5)\n",
        "# dbndiff2 = ((1.0/(n-1)) * torch.ones_like(bndiff2)) * dbnvar\n",
        "# dbndiff += 2 * bndiff * dbndiff2\n",
        "# dhprebn = bndiff.close()\n",
        "# dbnmeani = (-dbndiff).sum(0, keepdim=True)\n",
        "# dhprebn += 1/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# Next: Calculate dhprebn given dhpreact ( i.e. backprop through the batchnorm)\n",
        "# ( you'll also need to use some of the variables from the forward pass above)\n",
        "\n",
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjZJPL40OuvW",
        "outputId": "be63bdec-c1b6-40d7-e592-c04b229dc86b"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MAthematical expression for the Batch Norm backward pass\n",
        "https://www.notion.so/Mathematical-expression-for-Batch-Norm-backward-pass-2d6f8f908a6c4cda8d86af156f58db95"
      ],
      "metadata": {
        "id": "QABzo9YmWiUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 4: Putting it all together\n",
        "# Need to come back to this :-|"
      ],
      "metadata": {
        "id": "rYuDNqZbWrMK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}